import requests
from bs4 import BeautifulSoup
import pandas as pd
import yfinance as yf
import datetime

# Step 2: Scraping Nasdaq 100 Components from Wikipedia
url = 'https://en.wikipedia.org/wiki/Nasdaq-100'
response = requests.get(url)
soup = BeautifulSoup(response.content, 'html.parser')

# Find the table containing Nasdaq 100 components
table = soup.find('table', {'class': 'wikitable sortable'})

# Extract table header and data rows
header = [th.text.strip() for th in table.find_all('th')]
rows = []
for row in table.find_all('tr')[1:]:
    rows.append([td.text.strip() for td in row.find_all('td')])

# Create a Pandas DataFrame from the extracted data
df = pd.DataFrame(rows, columns=header)



tickers = df['Symbol'].tolist()

# Step 3: Downloading Historical Stock Data
start_date = (datetime.date.today() - datetime.timedelta(days=5*365)).strftime('%Y-%m-%d')
end_date = (datetime.date.today()).strftime('%Y-%m-%d')


for ticker in tickers:
    try:
        # Download stock data
        data = yf.download(ticker, start= start_date, end= end_date)
        if data.empty:
            print(f"Failed to get data for {ticker}: No data found or ticker possibly delisted.")
        else:
            # Save to CSV
            data.to_csv(f"{ticker}_data.csv")
            print(f"Data for {ticker} downloaded successfully.")
    except Exception as e:
        print(f"Failed to get ticker '{ticker}' reason: {e}")
# Select Adjusted Closing Prices
adj_close = data['Adj Close']

 #Step 4: Data Cleaning and Preparation
# Handling missing values by forward filling
adj_close = adj_close.ffill()



# Calculate daily log returns
daily_returns = adj_close.pct_change().dropna()
